{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "import time , random\n",
    "import mediapipe as mp\n",
    "import copy\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Dropout,BatchNormalization,Input,Conv1D,MaxPooling1D,\\\n",
    "                                    TimeDistributed,Activation,Lambda,ReLU,Conv1D,ConvLSTM1D,Flatten\n",
    "from tensorflow.keras.callbacks import TensorBoard,ModelCheckpoint,EarlyStopping\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import multilabel_confusion_matrix, accuracy_score,confusion_matrix\n",
    "from tensorflow.keras.optimizers import Adam,RMSprop\n",
    "from tensorflow.python.client import device_lib \n",
    "from tensorflow.keras.utils import plot_model\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 18392351820812195056\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 4922343424\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 8180013878408938606\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1060 with Max-Q Design, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Check GPU Existing\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Keypoints using MP Holistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_holistic = mp.solutions.holistic # Holistic model\n",
    "mp_drawing = mp.solutions.drawing_utils # Drawing utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mediapipe_detection(image, model):\n",
    "    \"\"\"\n",
    "    inputs: CV2 Image\n",
    "    output: Image, detected Landmarks\n",
    "    \"\"\"\n",
    "    \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) # COLOR CONVERSION BGR 2 RGB\n",
    "    image.flags.writeable = False                  # Image is no longer writeable to Improve Perf.\n",
    "    results = model.process(image)                 # Make prediction\n",
    "    image.flags.writeable = True                   # Image is now writeable \n",
    "    image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR) # COLOR COVERSION RGB 2 BGR\n",
    "    return image, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Landmarks on Image\n",
    "def draw_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    inputs: CV2 Image , Landmarks[Model Results]\n",
    "    \"\"\"\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION) # Draw face connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS) # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS) # Draw right hand connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw Styled Landmarks on Image\n",
    "def draw_styled_landmarks(image, results):\n",
    "    \"\"\"\n",
    "    inputs: CV2 Image , Landmarks[Model Results]\n",
    "    \"\"\"\n",
    "    # Draw face connections\n",
    "\n",
    "    mp_drawing.draw_landmarks(image, results.face_landmarks, mp_holistic.FACEMESH_TESSELATION, \n",
    "                             mp_drawing.DrawingSpec(color=(80,110,10), thickness=1, circle_radius=1), \n",
    "                             mp_drawing.DrawingSpec(color=(80,256,121), thickness=1, circle_radius=1)\n",
    "                             ) \n",
    "    # Draw pose connections\n",
    "    mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_holistic.POSE_CONNECTIONS,\n",
    "                             mp_drawing.DrawingSpec(color=(80,22,10), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(80,44,121), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw left hand connections\n",
    "    mp_drawing.draw_landmarks(image, results.left_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "                             ) \n",
    "    # Draw right hand connections  \n",
    "    mp_drawing.draw_landmarks(image, results.right_hand_landmarks, mp_holistic.HAND_CONNECTIONS, \n",
    "                             mp_drawing.DrawingSpec(color=(245,117,66), thickness=2, circle_radius=4), \n",
    "                             mp_drawing.DrawingSpec(color=(245,66,230), thickness=2, circle_radius=2)\n",
    "                             ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Extract Keypoint Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Keypoints from Landmarks And Concatenate it in One Array\n",
    "def extract_keypoints(results):\n",
    "    \"\"\"\n",
    "    inputs: Resutls from MediaPipe Model\n",
    "    output: Concatenated Landmarks\n",
    "    \"\"\"\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "    return np.concatenate([pose, face, lh, rh]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Setup Folders for Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our Actions\n",
    "fifty_actions = np.array(['All',\n",
    "'Boy',\n",
    "'Girl',\n",
    "'Book',\n",
    "'Mobile',\n",
    "'Car',\n",
    "'Baby',\n",
    "'Door',\n",
    "'End',\n",
    "'Eat',\n",
    "'Drink',\n",
    "'Father',\n",
    "'Mother',\n",
    "'Go',\n",
    "'Good',\n",
    "'Bad',\n",
    "'Food',\n",
    "'House',\n",
    "'In',\n",
    "'Out',\n",
    "'Sad',\n",
    "'Happy',\n",
    "'Man',\n",
    "'Woman',\n",
    "'Stop',\n",
    "'School',\n",
    "'New',\n",
    "'Old',\n",
    "'Play',\n",
    "'Room',\n",
    "'See',\n",
    "'Sit',\n",
    "'Sister',\n",
    "'Brother',\n",
    "'Think',\n",
    "'Work',\n",
    "'name',\n",
    "'Yes',\n",
    "'No',\n",
    "'Walk',\n",
    "'Love',\n",
    "'Need',\n",
    "'Respect',\n",
    "'Money',\n",
    "'I',\n",
    "'You',\n",
    "'Day',\n",
    "'Ambulance',\n",
    "'Buy',\n",
    "'Bread',])\n",
    "sorted_actions=sorted(fifty_actions)\n",
    "ten_actions=np.array(['Behind','Brother','Egypt','Father','Front of','Hello','How are you','I','Mother'\n",
    ",'Sister'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preprocess Data and Create Labels and Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labeling Actions to Numbers\n",
    "label_map = {label:num for num, label in enumerate(sorted_actions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'All': 0,\n",
       " 'Ambulance': 1,\n",
       " 'Baby': 2,\n",
       " 'Bad': 3,\n",
       " 'Book': 4,\n",
       " 'Boy': 5,\n",
       " 'Bread': 6,\n",
       " 'Brother': 7,\n",
       " 'Buy': 8,\n",
       " 'Car': 9,\n",
       " 'Day': 10,\n",
       " 'Door': 11,\n",
       " 'Drink': 12,\n",
       " 'Eat': 13,\n",
       " 'End': 14,\n",
       " 'Father': 15,\n",
       " 'Food': 16,\n",
       " 'Girl': 17,\n",
       " 'Go': 18,\n",
       " 'Good': 19,\n",
       " 'Happy': 20,\n",
       " 'House': 21,\n",
       " 'I': 22,\n",
       " 'In': 23,\n",
       " 'Love': 24,\n",
       " 'Man': 25,\n",
       " 'Mobile': 26,\n",
       " 'Money': 27,\n",
       " 'Mother': 28,\n",
       " 'Need': 29,\n",
       " 'New': 30,\n",
       " 'No': 31,\n",
       " 'Old': 32,\n",
       " 'Out': 33,\n",
       " 'Play': 34,\n",
       " 'Respect': 35,\n",
       " 'Room': 36,\n",
       " 'Sad': 37,\n",
       " 'School': 38,\n",
       " 'See': 39,\n",
       " 'Sister': 40,\n",
       " 'Sit': 41,\n",
       " 'Stop': 42,\n",
       " 'Think': 43,\n",
       " 'Walk': 44,\n",
       " 'Woman': 45,\n",
       " 'Work': 46,\n",
       " 'Yes': 47,\n",
       " 'You': 48,\n",
       " 'name': 49}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data 60 fps then Transform it to 30fps\n",
    "sequences1, labels1 = [], []\n",
    "DATA_PATH_KEYPOINTS = os.path.join(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\MP_Enhanced_Data_KEYPOINTS')\n",
    "for action in fifty_actions:\n",
    "    list_seq = glob.glob('/'.join([DATA_PATH_KEYPOINTS,action,'*']))\n",
    "    for sequence in list_seq:       \n",
    "        window = []\n",
    "        last_frame=[]\n",
    "        # Take two Steps to Convert it To 30FPS\n",
    "        for frame_num in range(0,60,2):\n",
    "            #Load Frame Keypoints\n",
    "            if os.path.exists(os.path.join(sequence, \"{}.npy\".format(frame_num))):\n",
    "                \n",
    "                res1 = np.load(os.path.join(sequence, \"{}.npy\".format(frame_num)))\n",
    "                #Take Hands Landmarks\n",
    "                lh_rh = res1[1536:]\n",
    "                #Remove z Axis From Landmarks\n",
    "                for z in range(2,lh_rh.shape[0],3):\n",
    "                        lh_rh[z] = None\n",
    "                #Remove NaN Data\n",
    "                lh_rh = lh_rh[np.logical_not(np.isnan(lh_rh))]\n",
    "                is_all_zero = np.all((lh_rh == 0))\n",
    "                if is_all_zero:\n",
    "                    if len(last_frame)==0:\n",
    "                        window.append([0 for i in range(84)])\n",
    "                        continue\n",
    "                    window.append(last_frame)\n",
    "                    continue\n",
    "                last_frame=lh_rh\n",
    "                window.append(lh_rh)\n",
    "            else:\n",
    "                if len(last_frame)==0:\n",
    "                    window.append([0 for i in range(84)])\n",
    "                    continue\n",
    "                window.append(last_frame)\n",
    "            \n",
    "            \n",
    "        sequences1.append(window)\n",
    "        labels1.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2250, 30, 84), (2250, 50))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert Lists To Array\n",
    "X2 = np.array(sequences1)\n",
    "#Convert Labels to OHE\n",
    "y2 = to_categorical(labels1).astype(int)\n",
    "X2.shape, y2.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load 30 Fps Data Folder \n",
    "DATA_PATH_KEYPOINTS = os.path.join(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\AllData')\n",
    "sequences, labels = [], []\n",
    "for action in ten_actions:\n",
    "    list_seq = glob.glob('/'.join([DATA_PATH_KEYPOINTS,'30FPS',action,'*']))\n",
    "    for sequence in list_seq:       \n",
    "        window = []\n",
    "       \n",
    "        for frame_num in range(0,30):\n",
    "            if os.path.exists(os.path.join(sequence, \"{}.npy\".format(frame_num))):\n",
    "                #Load Frame Keypoints\n",
    "                res1 = np.load(os.path.join(str(sequence), \"{}.npy\".format(frame_num)))\n",
    "                #Take Hands Landmarks\n",
    "                lh_rh = res1[1536:]\n",
    "                #Remove z Axis From Landmarks\n",
    "                for z in range(2,lh_rh.shape[0],3):\n",
    "                        lh_rh[z] = None\n",
    "                #Remove NaN Data\n",
    "                lh_rh = lh_rh[np.logical_not(np.isnan(lh_rh))]\n",
    "                is_all_zero = np.all((lh_rh == 0))\n",
    "\n",
    "                window.append(lh_rh)\n",
    "            else:\n",
    "                window.append([0 for i in range(84)])\n",
    "            \n",
    "        sequences.append(window)\n",
    "        labels.append(label_map[action])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((570, 30, 84), (570, 10))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert Lists To Array\n",
    "X1 = np.array(sequences)\n",
    "#Convert Labels to OHE\n",
    "y1 = to_categorical(labels).astype(int)\n",
    "X1.shape, y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1110, 30, 84), (1110, 10))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all data\n",
    "X = np.concatenate([X1,X2])\n",
    "y = np.concatenate([y1,y2])\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save data for ease of use\n",
    "np.save(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\X_data',X)\n",
    "np.save(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\y_data',y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1110, 30, 84), (1110, 10))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data from saved file \n",
    "X=np.load(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\X_data.npy')\n",
    "y=np.load(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\y_data.npy')\n",
    "X.shape,y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotation Augmentation\n",
    "def augment_data_rotataion(X,y):\n",
    "    '''\n",
    "    input: X,y  as numpy array Shape: [Samples,Timesteps,Features]\n",
    "    output: Augmented X,y as numpy array Shape:[Samples,Timesteps,Features]\n",
    "    '''\n",
    "    # Make an Array with Shape Like Original One\n",
    "    augmented_X = np.zeros_like(X)\n",
    "    augmented_y = np.zeros_like(y)\n",
    "    \n",
    "    #Looping in all Examples\n",
    "    for ex in range(X.shape[0]):\n",
    "        # Get Random Angle Betwwen -5,5\n",
    "        rotation_angle = random.randint(-5,5)\n",
    "        # Convert it to Radians\n",
    "        theta = np.radians(rotation_angle)\n",
    "        c, s = np.cos(theta), np.sin(theta)\n",
    "        # Build a Rotation Matrix\n",
    "        rotation_matrix = np.array(((c, -s), (s, c)))\n",
    "        # Looping Each Frame\n",
    "        for frame in range(X.shape[1]):\n",
    "            window = []\n",
    "            # looping each Point within Frame\n",
    "            for i in range(0,X.shape[2]-1,2):\n",
    "                # Get Keypoint\n",
    "                keypoint = np.array([X[ex][frame][i],X[ex][frame][i+1]])\n",
    "                # Calculate Rotated Keypoint\n",
    "                rotated_keypoint = np.dot(rotation_matrix, keypoint)\n",
    "                keypoint_x = rotated_keypoint[0]\n",
    "                keypoint_y = rotated_keypoint[1]\n",
    "                # Append New Keypoint To our Data\n",
    "                window.extend([keypoint_x,keypoint_y])\n",
    "            augmented_X[ex][frame] = np.array(window)\n",
    "        augmented_y[ex] = y[ex]\n",
    "    return augmented_X,augmented_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Augmentation\n",
    "def augment_data_scale(X,y):\n",
    "    '''\n",
    "    input: X,y  as numpy array Shape: [Samples,Timesteps,Features]\n",
    "    output: Augmented X,y as numpy array Shape: [Samples,Timesteps,Features]\n",
    "    '''\n",
    "    # Make an Array with Shape Like Original One\n",
    "    augmented_X = np.zeros_like(X)\n",
    "    augmented_y = np.zeros_like(y)\n",
    "    # Looping in Each Sample\n",
    "    for ex in range(X.shape[0]):\n",
    "        # Get Random Scale Factor\n",
    "        SCALE = round(random.random(),2)\n",
    "        for frame in range(X.shape[1]):\n",
    "            # Calculate New Point\n",
    "            augmented_X[ex][frame] = X[ex][frame]*SCALE\n",
    "        augmented_y[ex] = y[ex]\n",
    "    return augmented_X,augmented_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augmented Rotated Data\n",
    "rot_x,rot_y = augment_data_rotataion(X2,y2) \n",
    "# Augmented Scaled Data\n",
    "scaled_x,scaled_y = augment_data_scale(X2,y2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6750, 30, 84), (6750, 50))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# concatenate all data [Original and Augmented]\n",
    "X_ = np.concatenate([X2,rot_x,scaled_x])\n",
    "y_ = np.concatenate([y2,rot_y,scaled_y])\n",
    "X_.shape,y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data in .npy files for ease of usage\n",
    "np.save(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\X50_aug_data',X_)\n",
    "np.save(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\y50_aug_data',y_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6750, 30, 84), (6750, 50))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_=np.load(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\X50_aug_data.npy')\n",
    "y_=np.load(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\y50_aug_data.npy')\n",
    "X_.shape,y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_, y_, test_size=0.20,shuffle=True,stratify=y_,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5400, 30, 84), (1350, 30, 84), (5400, 50), (1350, 50))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Build and Train Neural Network Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define The Time Steps\n",
    "timesteps = X_.shape[1]\n",
    "# Define The Features No.\n",
    "features = X_.shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Build a LSTM Model Arch\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(LSTM(64, return_sequences=True, activation='relu', input_shape=(timesteps,features))) # frames * Features\n",
    "model_lstm.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model_lstm.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model_lstm.add(Dense(64, activation='relu'))\n",
    "model_lstm.add(Dense(32, activation='relu'))\n",
    "model_lstm.add(Dense(actions.shape[0], activation='softmax'))\n",
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilation Configuration\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=1e-4), loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Callbacks\n",
    "log_dir = os.path.join('Logs/LSTM3')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/LSTM3.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=100)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lstm.fit(X_train, y_train, epochs=2000, callbacks=[callbacks],batch_size=32,validation_data=(X_test,y_test),initial_epoch = 13)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Conv1d + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_convlstm = Sequential()\n",
    "\n",
    "model_convlstm.add(Input(shape=(timesteps,features))) \n",
    "model_convlstm.add(Conv1D(128, # Filters\n",
    "                 5, # Kernel Size\n",
    "                 padding='same',\n",
    "                 activation='relu',\n",
    "                 strides=1))\n",
    "model_convlstm.add(MaxPooling1D(pool_size=4))\n",
    "model_convlstm.add(LSTM(64, return_sequences=False))\n",
    "model_convlstm.add(Dense(32, activation='relu'))\n",
    "model_convlstm.add(Dense(len(sorted_actions)))\n",
    "\n",
    "\n",
    "\n",
    "model_convlstm.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs/Conv1d_LSTM_custom')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/Conv1d_LSTM_custom.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=50)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convlstm.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_convlstm.fit(X_train, y_train, epochs=2000, callbacks=[callbacks],batch_size=4,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 CONVLSTM1D TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (samples, time, rows, channels)\n",
    "X_train_reshaped = X_train.reshape([X_train.shape[0],X_train.shape[1],X_train.shape[2],1])\n",
    "X_test_reshaped = X_test.reshape([X_test.shape[0],X_train.shape[1],X_train.shape[2],1])\n",
    "X_train_reshaped.shape,X_test_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d_lstm_tf = Sequential()\n",
    "\n",
    "model_conv1d_lstm_tf.add(Input(shape=(X_train.shape[1],X_train.shape[2],1))) # time steps(frames) * features\n",
    "model_conv1d_lstm_tf.add(ConvLSTM1D(filters=64,kernel_size=(5), data_format='channels_last', padding = 'same'\n",
    "                      ,return_sequences=True))\n",
    "model_conv1d_lstm_tf.add(ConvLSTM1D(filters=32,kernel_size=(5), data_format='channels_last', padding = 'same'\n",
    "                      ,return_sequences=False))\n",
    "model_conv1d_lstm_tf.add(MaxPooling1D((32)))\n",
    "#model2.add(Dense(32, activation='relu'))\n",
    "model_conv1d_lstm_tf.add(TimeDistributed(Dense(actions.shape[0], activation='relu')))\n",
    "model_conv1d_lstm_tf.add(Flatten())\n",
    "model_conv1d_lstm_tf.add(Dense(actions.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "model_conv1d_lstm_tf.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs/Conv1d_LSTM_tf')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/model_conv1d_lstm_tf.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=50)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d_lstm_tf.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d_lstm_tf.fit(X_train_reshaped, y_train,callbacks=[callbacks], epochs=2000, batch_size=1,validation_data=(X_test_reshaped,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Conv1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d = Sequential()\n",
    "model_conv1d.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1],X_train.shape[2])))\n",
    "model_conv1d.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "model_conv1d.add(Dropout(0.5))\n",
    "model_conv1d.add(MaxPooling1D(pool_size=2))\n",
    "model_conv1d.add(Flatten())\n",
    "model_conv1d.add(Dense(100, activation='relu'))\n",
    "model_conv1d.add(Dense(actions.shape[0], activation='softmax'))\n",
    "model_conv1d.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs/Conv1d')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/model_conv1d.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=50)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv1d.fit(X_train, y_train, epochs=2000, callbacks=[callbacks],batch_size=1,validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 With CTC Loss & Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CTCLayer(layers.Layer):\n",
    "    def __init__(self, name=None):\n",
    "        super().__init__(name=name)\n",
    "        self.loss_fn = keras.backend.ctc_batch_cost\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        # Compute the training-time loss value and add it\n",
    "        # to the layer using `self.add_loss()`.\n",
    "        batch_len = tf.cast(tf.shape(y_true)[0], dtype=\"int64\")\n",
    "        input_length = tf.cast(tf.shape(y_pred)[1], dtype=\"int64\")\n",
    "        label_length = tf.cast(tf.shape(y_true)[1], dtype=\"int64\")\n",
    "\n",
    "        input_length = input_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "        label_length = label_length * tf.ones(shape=(batch_len, 1), dtype=\"int64\")\n",
    "\n",
    "        loss = self.loss_fn(y_true, y_pred, input_length, label_length)\n",
    "        self.add_loss(loss)\n",
    "\n",
    "        # At test time, just return the computed predictions\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "\n",
    "    input_seq = layers.Input(\n",
    "            shape=(timesteps, features), name=\"seq\", dtype=\"float32\"\n",
    "        )\n",
    "    labels = layers.Input(name=\"label\", shape=(None,), dtype=\"int32\")\n",
    "\n",
    "    x = layers.Bidirectional(layers.LSTM(128, return_sequences=True, dropout=0.25))(input_seq)\n",
    "    #x = layers.BatchNormalization(name=\"lstm_1_bn\")(x)\n",
    "    #x = layers.ReLU(name=\"lstm_1_relu\")(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True, dropout=0.25))(x)\n",
    "    #x = layers.BatchNormalization(name=\"lstm_2_bn\")(x)\n",
    "    #x = layers.ReLU(name=\"lstm_2_relu\")(x)\n",
    "    #x = Dense(64, activation='relu')(x)\n",
    "\n",
    "    # Output layer\n",
    "    x = layers.Dense(actions.shape[0]+1, activation=\"softmax\", name=\"dense2\")(x)\n",
    "\n",
    "    # Add CTC layer for calculating CTC loss at each step\n",
    "    output = CTCLayer(name=\"ctc_loss\")(labels, x)\n",
    "\n",
    "    # Define the model\n",
    "    model = keras.models.Model(\n",
    "        inputs=[input_seq, labels], outputs=output, name=\"ctc_model_v1\"\n",
    "    )\n",
    "    # Optimizer\n",
    "    # Compile the model and return\n",
    "    model.compile(optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ctc_model = build_model()\n",
    "ctc_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(images, labels, train_size=0.9, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images)\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_valid, y_valid = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_valid, y_train, y_valid\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "x_train, x_valid, y_train, y_valid = split_data(np.array(X2), np.array(labels1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_single_sample(seq, label):\n",
    "    return {\"seq\": seq, \"label\": label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = (\n",
    "    train_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((x_valid, y_valid))\n",
    "validation_dataset = (\n",
    "    validation_dataset.map(\n",
    "        encode_single_sample, num_parallel_calls=tf.data.AUTOTUNE\n",
    "    )\n",
    "    .batch(batch_size)\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epochs = 500\n",
    "early_stopping_patience = 300\n",
    "# Add early stopping\n",
    "early_stopping = keras.callbacks.EarlyStopping(\n",
    "    monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True\n",
    ")\n",
    "log_dir = os.path.join('Logs/CTC')\n",
    "tb_callback = TensorBoard(log_dir=log_dir) ## !tensorboard --logdir=.\n",
    "# Train the model\n",
    "history = ctc_model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[early_stopping,tb_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_model = keras.models.Model(\n",
    "    ctc_model.get_layer(name=\"seq\").input, ctc_model.get_layer(name=\"dense2\").output\n",
    ")\n",
    "prediction_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function to decode the output of the network\n",
    "def decode_batch_predictions(pred):\n",
    "    input_len = np.ones(pred.shape[0]) * pred.shape[1]\n",
    "    # Use greedy search. For complex tasks, you can use beam search\n",
    "    results = keras.backend.ctc_decode(pred, input_length=input_len, greedy=True)[0][0][\n",
    "        :, :max_length\n",
    "    ]\n",
    "    prob = np.array(keras.backend.ctc_decode(pred, input_length=input_len, greedy=True,)[1][0][0]) #[0])#[:, :19])\n",
    "    # Iterate over the results and get back the text\n",
    "    output_text = []\n",
    "    for res in results:\n",
    "        #res = tf.strings.reduce_join(res).numpy().decode(\"utf-8\")\n",
    "        output_text.append(res)\n",
    "    return output_text , prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Let's check results on some validation samples\n",
    "predss=[]\n",
    "for batch in validation_dataset.take(10000):\n",
    "    batch_seq = batch[\"seq\"]\n",
    "    batch_labels = batch[\"label\"]\n",
    "    \n",
    "    preds = prediction_model.predict(batch_seq)\n",
    "    pred_texts = decode_batch_predictions(preds)[0]\n",
    "    predss.append((pred_texts == batch_labels)[0][0].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(predss)/len(predss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.6 Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    # Normalization and Attention\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(x, x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    res = x + inputs\n",
    "\n",
    "    # Feed Forward Part\n",
    "    x = layers.LayerNormalization(epsilon=1e-6)(res)\n",
    "    x = layers.Conv1D(filters=ff_dim, kernel_size=1, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(x)\n",
    "    return x + res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(\n",
    "    input_shape,\n",
    "    head_size,\n",
    "    num_heads,\n",
    "    ff_dim,\n",
    "    num_transformer_blocks,\n",
    "    mlp_units,\n",
    "    dropout=0,\n",
    "    mlp_dropout=0,\n",
    "):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_transformer_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "\n",
    "    x = layers.GlobalAveragePooling1D(data_format=\"channels_first\")(x)\n",
    "    for dim in mlp_units:\n",
    "        x = layers.Dense(dim, activation=\"relu\")(x)\n",
    "        x = layers.Dropout(mlp_dropout)(x)\n",
    "    outputs = layers.Dense(actions.shape[0], activation=\"softmax\")(x)\n",
    "    return keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model_transformer = build_model(\n",
    "    input_shape,\n",
    "    head_size=256,\n",
    "    num_heads=4,\n",
    "    ff_dim=4,\n",
    "    num_transformer_blocks=4,\n",
    "    mlp_units=[128],\n",
    "    mlp_dropout=0.4,\n",
    "    dropout=0.25,\n",
    ")\n",
    "model_transformer.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=Adam(learning_rate=1e-5),\n",
    "    metrics=[\"categorical_accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = os.path.join('Logs/transformer_encoder')\n",
    "tb_callback = TensorBoard(log_dir=log_dir,histogram_freq=1,\n",
    "                          update_freq='epoch',\n",
    "                          profile_batch=0) ## !tensorboard --logdir=.\n",
    "mc = ModelCheckpoint('Models/model_transformer_encoder.h5', monitor='val_categorical_accuracy', mode='max', verbose=1,save_best_only=True)\n",
    "es = EarlyStopping(monitor='val_categorical_accuracy', mode='max', verbose=1,patience=100)\n",
    "callbacks = [tb_callback,mc,es]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\signara\\Signara-main\\Sign To Text\\Notebooks\\Sign Lang Translator Word Based.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/signara/Signara-main/Sign%20To%20Text/Notebooks/Sign%20Lang%20Translator%20Word%20Based.ipynb#ch0000001?line=0'>1</a>\u001b[0m model_transformer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mload_model(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mD:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msignara\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSignara-main\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mSign To Text\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mutils\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "model_transformer=tf.keras.models.load_model(r'D:\\signara\\Signara-main\\Sign To Text\\utils\\model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=2000,\n",
    "    batch_size=8,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transformer.evaluate(X_test, y_test, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(model_transformer.predict(np.expand_dims(X_test[87],axis=0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_test[87])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = model.predict(X_test)\n",
    "np.argmax(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(res[4])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions[np.argmax(y_test[4])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Save and Load Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('action.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model_transformer_encoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Evaluation using Confusion Matrix and Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat = model_lstm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrue = np.argmax(y_test, axis=1).tolist()\n",
    "yhat = np.argmax(yhat, axis=1).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.heatmap(confusion_matrix(ytrue, yhat),annot =True,yticklabels=actions,xticklabels=actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(ytrue, yhat)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7edee622b834293ee779dd8d6917a1d3a6cc56ca2a446a7f95b5883c4f6a43a7"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit (system)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
